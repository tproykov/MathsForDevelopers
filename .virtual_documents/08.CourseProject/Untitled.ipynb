





# Uncomment if needed:
# !pip install pandas scikit-learn matplotlib



import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
%matplotlib inline






# Adjust the filename/path if necessary
DATA_PATH = "train.csv"

# Load the dataset
df = pd.read_csv(DATA_PATH)

# Quick overview
print(f"Total rows: {df.shape[0]}, columns: {df.shape[1]}")
print(df.head(3))






df['title'] = df['title'].fillna('')
df['text']  = df['text'].fillna('')
df['content'] = df['title'] + " " + df['text']

# Confirm
print(df[['content','label']].sample(3))






vectorizer = TfidfVectorizer(
    stop_words='english',
    max_features=5000  # keep the 5,000 most frequent terms
)

# Fit on entire corpus and transform
X = vectorizer.fit_transform(df['content'])
y = df['label'].values

print(f"TF-IDF matrix shape: {X.shape}")
# e.g., (number_of_articles, 5000)






X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)

print("Training set:", X_train.shape, y_train.shape)
print("Test set:",     X_test.shape,  y_test.shape)






model = LogisticRegression(
    solver='lbfgs',    # default solver; works well for many problems
    max_iter=1000,     # increase if the solver doesn’t converge
    n_jobs=-1          # use all CPU cores for faster training
)

# Train on the TF-IDF features
model.fit(X_train, y_train)
print("Model training complete.")






# 6.1. Predictions
y_pred = model.predict(X_test)

# 6.2. Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy on test set: {acc:.2%}")

# 6.3. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# 6.4. Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=["Real", "Fake"]))






import seaborn as sns  # seaborn is optional but makes plotting nicer

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
            xticklabels=["Pred_Real","Pred_Fake"],
            yticklabels=["True_Real","True_Fake"])
plt.ylabel("True label")
plt.xlabel("Predicted label")
plt.title("Confusion Matrix")
plt.show()






# Get feature names (terms) from the vectorizer
terms = vectorizer.get_feature_names_out()

# Get the coefficient values for the single logistic regression class (shape: [5000])
coefs = model.coef_[0]

# Pair terms with their coefficient, and sort by coefficient descending
coef_df = pd.DataFrame({
    'term': terms,
    'coef': coefs
}).sort_values(by='coef', ascending=False)

# Top 10 terms most indicative of Fake
print("Top 10 terms indicative of Fake news:")
print(coef_df.head(10))

# Top 10 terms most indicative of Real news (most negative coefficients)
print("\nTop 10 terms indicative of Real news:")
print(coef_df.tail(10))






import joblib

# Save both the vectorizer and the model
joblib.dump(vectorizer, "tfidf_vectorizer.joblib")
joblib.dump(model,       "logreg_fake_news_model.joblib")

print("Saved vectorizer and model to disk.")






# Load saved objects
loaded_vectorizer = joblib.load("tfidf_vectorizer.joblib")
loaded_model      = joblib.load("logreg_fake_news_model.joblib")

# Example new samples
new_articles = [
    "Breaking news: Scientists discover cure for common cold...",
    "Exclusive: Celebrity endorses miracle diet pill, doctors shocked..."
]

# Transform with TF-IDF
X_new = loaded_vectorizer.transform(new_articles)

# Predict
predictions = loaded_model.predict(X_new)
probabilities = loaded_model.predict_proba(X_new)[:,1]  # Probability of class ‘Fake’

for text, pred, prob in zip(new_articles, predictions, probabilities):
    label = "Fake" if pred == 1 else "Real"
    print(f"\nArticle: {text[:60]}...")
    print(f"Predicted label: {label} (prob_fake = {prob:.2f})")














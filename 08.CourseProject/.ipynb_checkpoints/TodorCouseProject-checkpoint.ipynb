{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f3b446-5b25-4330-bb2e-ad192c8dc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a3c74-8813-4acf-a8a4-ea81082b373a",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 1.1 Формулиране на изследователски въпрос\n",
    "\n",
    "> **Как можем да използваме математически модели за измерване на семантично сходство между текстове, с цел предварителна фактологична проверка (fact-checking)?**\n",
    "\n",
    "Целта на този проект е да приложи класически математически инструменти (TF-IDF, векторно представяне, косинусова прилика) за сравнение на текстове и оценка на тяхната прилика. Проектът се фокусира върху математиката, а не върху машинното обучение.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 1.2 Методи за семантично сравнение на текст\n",
    "\n",
    "### 🔹 Term Frequency (TF)\n",
    "\n",
    "\\[\n",
    "TF(t, d) = \\frac{f_{t,d}}{\\sum_k f_{k,d}}\n",
    "\\]\n",
    "\n",
    "- \\( f_{t,d} \\): честота на дума _t_ в документ _d_  \n",
    "- \\( \\sum_k f_{k,d} \\): общ брой думи в документа\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Inverse Document Frequency (IDF)\n",
    "\n",
    "\\[\n",
    "IDF(t, D) = \\log\\left( \\frac{N}{1 + |\\{d \\in D : t \\in d\\}|} \\right)\n",
    "\\]\n",
    "\n",
    "- \\( N \\): общ брой документи  \n",
    "- \\( |\\{d \\in D : t \\in d\\}| \\): брой документи, съдържащи думата _t_\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 TF–IDF\n",
    "\n",
    "\\[\n",
    "TF\\text{-}IDF(t, d, D) = TF(t,d) \\times IDF(t,D)\n",
    "\\]\n",
    "\n",
    "Тази стойност оценява колко важна е дадена дума за конкретен документ в контекста на цялата колекция.\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 1.3 Векторно представяне и косинусова прилика\n",
    "\n",
    "След изчисляване на TF-IDF, всеки документ може да бъде представен като вектор. Сходството между два вектора \\( \\vec{A} \\) и \\( \\vec{B} \\) се измерва чрез:\n",
    "\n",
    "\\[\n",
    "\\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\cdot ||\\vec{B}||}\n",
    "\\]\n",
    "\n",
    "- \\( \\vec{A} \\cdot \\vec{B} \\): скаларно произведение  \n",
    "- \\( ||\\vec{A}|| \\): Евклидова норма на вектор _A_\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 1.4 Математически основи\n",
    "\n",
    "| Концепт                  | Приложение                                                 |\n",
    "|--------------------------|------------------------------------------------------------|\n",
    "| Вектори                  | Представяне на документи като числови редове               |\n",
    "| Скаларно произведение    | Оценка на близост между текстове                          |\n",
    "| Евклидова норма          | Нормализиране и изчисление на косинусова прилика           |\n",
    "| Логаритми                | Компонент от формулата за IDF                             |\n",
    "| Матрици                  | Представяне на множество документи (TF-IDF матрица)        |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 1.5 Структура на проекта\n",
    "\n",
    "1. **Въведение**  \n",
    "   - Мотивация  \n",
    "   - Проблематика  \n",
    "   - Цел и въпроси\n",
    "\n",
    "2. **Математическа теория**  \n",
    "   - TF, IDF, TF-IDF  \n",
    "   - Косинусова прилика  \n",
    "   - Векторно пространство\n",
    "\n",
    "3. **Примерен корпус от текстове**\n",
    "\n",
    "4. **Изчисления и визуализации**\n",
    "\n",
    "5. **Пример за проверка на нов текст**\n",
    "\n",
    "6. **Заключение и възможности за надграждане**\n",
    "\n",
    "---\n",
    "\n",
    "🧭 _Фаза 1 завършва с ясно формулирана теоретична рамка. Във Фаза 2 ще се пристъпи към създаване и обработка на малък корпус от текстове._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ac15e-dec2-47ad-854f-acec8ecdcbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b70f17-eb79-40de-9641-a100ef114be8",
   "metadata": {},
   "source": [
    "# 📘 Introduction and Literature Review\n",
    "\n",
    "## 1.1 Formulating the Research Question\n",
    "\n",
    "In the context of growing misinformation on social media, the need for tools that allow rapid verification of factual claims is increasingly pressing. This project is driven by the following research question:\n",
    "\n",
    "> **How can classical mathematical methods be used to assess semantic similarity between texts for the purpose of preliminary fact-checking?**\n",
    "\n",
    "This project explores the application of **TF-IDF** vectorization and **cosine similarity** as methods for evaluating semantic closeness between social media posts and a database of verified claims.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Foundations of Fact-Checking and Semantic Comparison\n",
    "\n",
    "Fact-checking typically involves three stages:  \n",
    "- identifying a claim,  \n",
    "- retrieving relevant evidence, and  \n",
    "- logically comparing the claim and evidence (Thorne & Vlachos, 2018).\n",
    "\n",
    "In automated systems, **semantic text similarity** methods play a key role in retrieving relevant information. Even in modern systems, **TF-IDF and cosine similarity** are frequently used in the retrieval phase for their speed, simplicity, and interpretability (Chen et al., 2017; Zeng et al., 2021).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Methods for Semantic Text Comparison\n",
    "\n",
    "### 🔹 Classical Approach: TF-IDF and Cosine Similarity\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) is a widely used technique for representing text as vectors. It combines the local importance of a word in a document (TF) with its inverse frequency across a corpus (IDF)  \n",
    "(Spärck Jones, 1972; Robertson, 2004):\n",
    "\n",
    "\\[\n",
    "TF\\text{-}IDF(t, d, D) = TF(t,d) \\times \\log\\left( \\frac{N}{1 + df(t)} \\right)\n",
    "\\]\n",
    "\n",
    "Once text is vectorized, semantic similarity can be measured using **cosine similarity**:\n",
    "\n",
    "\\[\n",
    "\\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\cdot ||\\vec{B}||}\n",
    "\\]\n",
    "\n",
    "Mihalcea et al. (2006) showed that such classical similarity methods are highly effective across various NLP tasks. Marcinczuk et al. (2021) further demonstrated that **TF-IDF can sometimes perform as well as or better than embedding-based methods** like Word2Vec and BERT, especially in constrained or low-resource settings.\n",
    "\n",
    "### 🔹 Comparison to Embedding Methods\n",
    "\n",
    "Embedding methods (e.g., BERT, Word2Vec) offer deeper contextual understanding but require more resources and data. According to Chandrasekaran & Mago (2021), **TF-IDF remains a well-justified choice** in early-stage engineering projects and educational settings due to its transparency and effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 References (APA Style)\n",
    "\n",
    "- Chen, D., Fisch, A., Weston, J., & Bordes, A. (2017). *Reading Wikipedia to answer open-domain questions*. Proceedings of ACL 2017. [Link](https://aclanthology.org/P17-1171/)\n",
    "- Chandrasekaran, D., & Mago, V. (2021). *Evolution of semantic similarity – A survey*. ACM Computing Surveys, 54(2). [Link](https://arxiv.org/abs/2004.13820)\n",
    "- Marcinczuk, M., et al. (2021). *Text document clustering: WordNet vs. TF-IDF vs. Word embeddings*. GWC 2021. [Link](https://aclanthology.org/2021.gwc-1.24.pdf)\n",
    "- Mihalcea, R., Corley, C., & Strapparava, C. (2006). *Corpus-based and knowledge-based measures of text semantic similarity*. AAAI 2006. [Link](https://aaai.org/Papers/AAAI/2006/AAAI06-123.pdf)\n",
    "- Robertson, S. (2004). *Understanding inverse document frequency: On theoretical arguments for IDF*. Journal of Documentation, 60(5). [Link](https://www.staff.city.ac.uk/~sbrp622/idfpapers/Robertson_idf_JDoc.pdf)\n",
    "- Spärck Jones, K. (1972). *A statistical interpretation of term specificity and its application in retrieval*. Journal of Documentation, 28(1). [Link](https://www.staff.city.ac.uk/~sbrp622/idfpapers/ksj_orig.pdf)\n",
    "- Thorne, J., & Vlachos, A. (2018). *Automated fact checking: Task formulations, methods and future directions*. COLING 2018. [Link](https://aclanthology.org/C18-1283/)\n",
    "- Zeng, X., Abumansour, A. S., & Zubiaga, A. (2021). *Automated fact-checking: A survey*. Language and Linguistics Compass, 15(10). [Link](https://arxiv.org/abs/2109.11427)\n",
    "\n",
    "---\n",
    "\n",
    "_Next step: creating and preprocessing a small corpus of sample texts (Phase 2)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1502e5-a6f1-407e-8264-5ddc1710f4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

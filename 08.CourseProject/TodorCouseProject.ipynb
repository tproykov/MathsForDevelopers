{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f3b446-5b25-4330-bb2e-ad192c8dc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75b8f0-270b-4faf-8ab9-98c2f626b1ed",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de23e8-03e1-471d-b83d-6fed0233f425",
   "metadata": {
    "citation-manager": {
     "citations": {
      "6hhjr": [
       {
        "id": "79625/YFIM6W6S",
        "source": "zotero"
       }
      ],
      "8kosb": [
       {
        "id": "79625/Z792DB78",
        "source": "zotero"
       }
      ],
      "h0n81": [
       {
        "id": "79625/Z62QSGRH",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 The Growing Need for Fact-Checking in Social Media\n",
    "\n",
    "In the digital era, social media platforms have become dominant sources of information for millions of users. However, the open nature and virality of these platforms have also made them fertile ground for the spread of misinformation and false claims. \n",
    "\n",
    "As noted by Zeng et al. <cite id=\"h0n81\"><a href=\"#zotero%7C79625%2FZ62QSGRH\">[1]</a></cite> \"the rise of social media has drastically increased the speed and reach of information dissemination, often bypassing traditional gatekeepers of truth such as journalists and editors.\" This shift has created a pressing demand for scalable, automated systems capable of evaluating the factual accuracy of content in real time. The objective is not merely to flag content post hoc but to build proactive mechanisms that can assist users, journalists, and platforms in identifying misleading information before it goes viral.\n",
    "\n",
    "Automated fact-checking has emerged as a promising solution to this challenge. \n",
    "\n",
    "According to Thorne & Vlachos <cite id=\"6hhjr\"><a href=\"#zotero%7C79625%2FYFIM6W6S\">[2]</a></cite>, automated fact-checking systems typically consist of three core stages: claim detection, evidence retrieval, and claim verification. While many recent advances in this area rely on deep learning techniques, classical mathematical methods such as **TF-IDF** and **cosine similarity** still play a crucial role in the early stages—particularly in retrieving semantically relevant candidate texts efficiently. \n",
    "\n",
    "As Chen et al. <cite id=\"8kosb\"><a href=\"#zotero%7C79625%2FZ792DB78\">[3]</a></cite> demonstrated in their open-domain QA system, TF-IDF-based retrieval remains a reliable and computationally lightweight approach to identifying supporting information from large knowledge sources like Wikipedia. These findings support the use of TF-IDF in educational and prototype systems where interpretability, speed, and mathematical transparency are critical.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0759cb0-1dbe-4ea1-afa5-d58f404d1ab8",
   "metadata": {},
   "source": [
    "## 1.2 The Research Question\n",
    "\n",
    "In the context of growing misinformation on social media, the need for tools that allow rapid verification of factual claims is increasingly pressing. This project is driven by the following research question:\n",
    "\n",
    "> **How can classical mathematical methods be used to assess semantic similarity between texts for the purpose of preliminary fact-checking?**\n",
    "\n",
    "This project explores the application of **TF-IDF** vectorisation and **cosine similarity** as methods for evaluating semantic closeness between social media posts and a database of verified claims. The rationale for using **TF-IDF** vectorisation and **cosine similarity** is described in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595dbd64-967f-464a-8ed8-9152fa9b6abd",
   "metadata": {
    "citation-manager": {
     "citations": {
      "5d20n": [
       {
        "id": "79625/Z62QSGRH",
        "source": "zotero"
       }
      ],
      "rl1xa": [
       {
        "id": "79625/Z62QSGRH",
        "source": "zotero"
       }
      ],
      "sw4j6": [
       {
        "id": "79625/YFIM6W6S",
        "source": "zotero"
       }
      ],
      "zpxoj": [
       {
        "id": "79625/Z792DB78",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "## 1.3 Foundations of Fact-Checking and Semantic Comparison\n",
    "\n",
    "Fact-checking is a structured process aimed at evaluating the truthfulness of a given claim based on available evidence. As outlined by *Thorne and Vlachos (2018)*, most computational fact-checking pipelines can be broken down into three fundamental stages:  \n",
    "1. **Claim detection**, where a potentially check-worthy statement is identified  \n",
    "2. **Evidence retrieval**, where supporting or contradicting information is located from trusted sources  \n",
    "3. **Claim verification**, where the semantic relationship between the claim and the evidence is assessed to determine the truth value <cite id=\"rl1xa\"><a href=\"#zotero%7C79625%2FZ62QSGRH\">[1]</a></cite> <cite id=\"sw4j6\"><a href=\"#zotero%7C79625%2FYFIM6W6S\">[2]</a></cite>.\n",
    "\n",
    "This pipeline reflects not only the logical reasoning humans engage in when evaluating information but also the operational structure that automated systems must emulate to perform reliable verification.\n",
    "\n",
    "A critical step in this pipeline is evidence retrieval, which depends heavily on the ability to identify **semantically similar** texts. The task is nontrivial: unlike keyword search, semantic similarity measures aim to capture conceptual overlap even when different surface forms are used. For example, the claim *“The president visited Berlin in 2023”* should be recognized as semantically related to a sentence like *“In 2023, the German capital welcomed the head of state.”* Such tasks demand methods that go beyond syntactic comparison and measure meaning, even in the absence of shared vocabulary.\n",
    "\n",
    "While modern deep learning models such as BERT have advanced the state of the art in semantic understanding, **classical mathematical approaches remain essential tools**, especially in the evidence retrieval phase. **TF-IDF** (Term Frequency–Inverse Document Frequency) is one of the most commonly used techniques for text vectorization, offering a computationally efficient way to represent documents numerically. When combined with **cosine similarity**, TF-IDF enables systems to retrieve texts that are most relevant to a given claim, by quantifying how closely related their term distributions are. This approach was notably used by Chen et al. <cite id=\"zpxoj\"><a href=\"#zotero%7C79625%2FZ792DB78\">[3]</a></cite> in their open-domain QA system, where TF-IDF was applied to quickly filter a large corpus of Wikipedia articles before deeper analysis.\n",
    "\n",
    "Zeng et al.<cite id=\"5d20n\"><a href=\"#zotero%7C79625%2FZ62QSGRH\">[1]</a></cite> highlight that even with the rise of neural models, TF-IDF and cosine similarity continue to serve as effective first-pass methods in hybrid systems. They offer a transparent and resource-efficient alternative, especially valuable in real-time environments or early-stage prototypes where training complex models is not feasible. These methods also align well with educational contexts, where the focus is on understanding underlying mathematical principles rather than relying entirely on pretrained models. As such, semantic comparison via TF-IDF is not only relevant but often preferable in settings where speed, interpretability, and low computational cost are priorities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f931e-10da-4b1e-a5f6-c6ba11cbbc30",
   "metadata": {
    "citation-manager": {
     "citations": {
      "ee99k": [
       {
        "id": "79625/QPZSI7AS",
        "source": "zotero"
       }
      ],
      "p0gle": [
       {
        "id": "79625/YS2A7YKH",
        "source": "zotero"
       }
      ],
      "pdr5j": [
       {
        "id": "79625/MDDH62P9",
        "source": "zotero"
       }
      ],
      "v2rio": [
       {
        "id": "79625/2P77V7I4",
        "source": "zotero"
       }
      ],
      "vs5yt": [
       {
        "id": "79625/MLYXVA9L",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "## 1.4 Methods for Semantic Text Comparison\n",
    "\n",
    "### 1.4.1 Classical approach: TF-IDF and Cosine similarity\n",
    "\n",
    "One of the most widely used classical techniques for representing textual data numerically is **TF-IDF** (Term Frequency–Inverse Document Frequency). This method assigns weight to each term in a document based on its frequency within the document (TF) and the inverse of its frequency across all documents in a corpus (IDF). The IDF component penalizes common words (e.g., “the”, “is”) and amplifies more informative, discriminative terms. This formulation, introduced by Spärck Jones<cite id=\"p0gle\"><a href=\"#zotero%7C79625%2FYS2A7YKH\">[4]</a></cite> and mathematically refined by Robertson<cite id=\"ee99k\"><a href=\"#zotero%7C79625%2FQPZSI7AS\">[5]</a></cite>, enables a simple yet powerful transformation of unstructured text into vectors suitable for quantitative analysis.\n",
    "\n",
    "#### 1.4.1.1 Term Frequency (TF)\n",
    "\n",
    "Measures how frequently a term appears in a document relative to the total number of terms in that document:\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{f_{t,d}}{\\sum_k f_{k,d}}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( f_{t,d} \\) = frequency of term \\( t \\) in document \\( d \\)  \n",
    "- \\( \\sum_k f_{k,d} \\) = total number of terms in document \\( d \\)\n",
    "\n",
    "#### 1.4.1.2 Inverse Document Frequency (IDF)\n",
    "\n",
    "Evaluates how \"unique\" or \"rare\" a term is across the entire document collection:\n",
    "\n",
    "$$\n",
    "IDF(t, D) = \\log\\left( \\frac{N}{1 + |\\{d \\in D : t \\in d\\}|} \\right)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( N \\) = total number of documents in the collection  \n",
    "- \\( |\\{d \\in D : t \\in d\\}| \\) = number of documents in which term \\( t \\) appears  \n",
    "\n",
    "> **Note:** The \"+1\" in the denominator is used to avoid division by zero.\n",
    "\n",
    "#### 1.4.1.3 TF-IDF\n",
    "\n",
    "Combines the two metrics to evaluate how important a term is to a specific document, while accounting for how common or rare it is across the full corpus:\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t, d, D) = TF(t,d) \\times IDF(t,D)\n",
    "$$\n",
    "\n",
    "Or,\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t, d, D) = TF(t,d) \\times \\log\\left( \\frac{N}{1 + df(t)} \\right)\n",
    "$$\n",
    "\n",
    "After text is converted into TF-IDF vectors, **cosine similarity** is commonly used to measure the semantic closeness between two documents. It calculates the cosine of the angle between two vectors in an n-dimensional space:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\cdot ||\\vec{B}||}\n",
    "$$\n",
    "\n",
    "A cosine value close to 1 indicates high similarity. This approach is computationally efficient, easy to implement, and interpretable—making it a popular baseline in many information retrieval and natural language processing (NLP) applications. Mihalcea et al. <cite id=\"pdr5j\"><a href=\"#zotero%7C79625%2FMDDH62P9\">[6]</a></cite> demonstrated that TF-IDF and cosine similarity are effective in a wide range of tasks, including summarization, question answering, and semantic similarity estimation. More recently, Marcińczuk et al. <cite id=\"v2rio\"><a href=\"#zotero%7C79625%2F2P77V7I4\">[7]</a></cite> showed that, in some constrained environments, TF-IDF methods can perform comparably or even outperform modern embedding-based techniques such as Word2Vec and BERT.\n",
    "\n",
    "\n",
    "### 1.4.2 Comparison to the more modern Embedding methods\n",
    "\n",
    "Modern alternatives to TF-IDF include **neural embedding methods** like Word2Vec, GloVe, Doc2Vec, and most notably BERT. These techniques rely on pretrained language models that capture context, syntax, and semantics by mapping words and sentences into dense, high-dimensional vectors based on their surrounding text. For example, BERT uses deep bidirectional transformers trained on massive corpora to understand nuanced context and polysemy, allowing for superior performance in many downstream NLP tasks.\n",
    "\n",
    "However, this power comes at a cost. Embedding methods are **computationally intensive**, often requiring specialized hardware (e.g., GPUs), large datasets, and familiarity with machine learning frameworks. Moreover, they are less interpretable than TF-IDF. While TF-IDF clearly highlights which terms contribute most to similarity, embeddings operate in a latent space that is difficult to intuitively understand or audit. As Chandrasekaran & Mago <cite id=\"vs5yt\"><a href=\"#zotero%7C79625%2FMLYXVA9L\">[8]</a></cite> point out in their comprehensive survey, embeddings offer significant advantages in terms of contextualization, but the trade-offs in terms of transparency and resource demands must be considered carefully.\n",
    "\n",
    "\n",
    "### 1.4.3 Why TF-IDF is used in this project\n",
    "\n",
    "Given the mathematical focus of this course and the goal of building a demonstrable prototype within a limited time frame, **TF-IDF combined with cosine similarity is a well-justified and appropriate choice**. It enables a rigorous yet transparent application of mathematical concepts such as logarithmic scaling, vector norms, and inner products—all foundational to the field of machine learning. In contrast, embedding methods—while powerful—abstract away many of the underlying mathematical details, making them less pedagogically valuable in an early-stage AI engineering curriculum.\n",
    "\n",
    "Additionally, the interpretability of TF-IDF provides a clear advantage in educational contexts. It is easy to trace how each word contributes to a similarity score, which not only aids in debugging but also facilitates deeper understanding of the connection between language and mathematics. The method is also **scalable, fast, and language-agnostic**, making it ideal for the limited dataset and controlled setting envisioned in this course project.\n",
    "\n",
    "For these reasons, TF-IDF will serve as the primary method for semantic comparison in this project, laying a solid mathematical foundation while leaving the door open to future enhancements using more complex NLP models in later stages of the AI engineering program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5062c04-ec91-4797-8338-411481d31a8c",
   "metadata": {},
   "source": [
    "# 2. Building a Mini-Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c7e5b-45e0-4189-8867-b2550ce798a6",
   "metadata": {},
   "source": [
    "## 2.1 Selecting the set of Social media claims\n",
    "\n",
    "For the purposes of this educational project, I will construct a mini-dataset of around 8 to 12 short text claims, simulating real-world social media posts. These claims should be labeled as either \"True\" or \"False\", with 4–6 examples per class. The goal is not quantity but clarity and control. The dataset should contain concise, varied claims with known veracity.\n",
    "\n",
    "Criteria for selecting the media posts for the mini-dataset:\n",
    "* Each post should be under 300 characters (to fit with traditional 'tweets', media headlines, comments in comments section, etc.\n",
    "* Use of informal phrasing where appropriate, to simulate real-world postings.\n",
    "* The \"False\" claims are verifiably incorrect, based on trusted sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe8878-2b49-4bac-b253-9407e26ee63e",
   "metadata": {},
   "source": [
    "## 2.2 Text preprocessing\n",
    "\n",
    "Once your mini-dataset is compiled, you will need to clean and tokenize the text to prepare it for vectorization. The goal is to reduce noise and standardize the structure while retaining meaning.\n",
    "\n",
    "Preprocessing steps:\n",
    "* Lowercasing – Convert all text to lowercase for uniformity.\n",
    "* Punctuation removal – Strip out punctuation marks (periods, commas, quotes, etc.) using regular expressions.\n",
    "* Tokenization – Split each text into a list of individual words or \"tokens\". Recommended tool: nltk.word_tokenize() or Python’s built-in split().\n",
    "* Stop word removal – Filter out common non-informative words (e.g., “the”, “is”, “and”). Recommended: Use NLTK’s built-in English stopword list:\n",
    "from nltk.corpus import stopwords\n",
    "* (Optional) Lemmatization or stemming – Reduces words to their base or root form. Use nltk.stem.WordNetLemmatizer() for lemmatization if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e26beb-fa3e-4803-95fa-01b55d37fba7",
   "metadata": {},
   "source": [
    "# 3. Mathematical Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554470a-3622-43f8-b090-8d09dcc78537",
   "metadata": {},
   "source": [
    "## 3.1 Calculating TF, IDF, and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d6f30-3512-4239-b113-8b5555258084",
   "metadata": {},
   "source": [
    "### 3.1.1 Mathematical Formulas\n",
    "\n",
    "**Term Frequency (TF):**\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{f_{t,d}}{\\sum_k f_{k,d}}\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency (IDF):**\n",
    "\n",
    "$$\n",
    "IDF(t, D) = \\log\\left( \\frac{N}{1 + |\\{d \\in D : t \\in d\\}|} \\right)\n",
    "$$\n",
    "\n",
    "**TF-IDF:**\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t, d, D) = TF(t,d) \\times IDF(t,D)\n",
    "$$\n",
    "\n",
    "### 3.1.2 Manual Calculation (Demonstration)\n",
    "\n",
    "- Choose 2–3 short sample documents (3–5 words each).\n",
    "- Show manual calculation of:\n",
    "  - Term frequency for each word\n",
    "  - IDF for each term in the corpus\n",
    "  - Resulting TF-IDF values for each word in each document\n",
    "- Present results in a simple table for clarity.\n",
    "\n",
    "\n",
    "### 3.1.3 Programmatic TF-IDF sing `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5704f2-dc6f-439e-9849-b7dab8426b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Climate change is real\",\n",
    "    \"The earth is flat\",\n",
    "    \"Global warming is a serious issue\",\n",
    "    \"The moon landing was faked\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e56f7-f03c-439e-8d0a-8d7fa2c8db53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4663f9-e475-4038-ba4a-fc2a02dcec62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00234436-54c9-4e77-8cba-0aa83c99cc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb411c-e45a-42f9-aa72-3e66f3df71a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7fc42dd-4bcf-41ec-a3ee-60e66c3464bf",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff82a14-4f8c-4f37-b9db-ae2a8f9e6cdd",
   "metadata": {},
   "source": [
    "<!-- BIBLIOGRAPHY START -->\n",
    "<div class=\"csl-bib-body\">\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/Z62QSGRH\"></i>\n",
    "    <div class=\"csl-left-margin\">[1]</div><div class=\"csl-right-inline\">X. Zeng, A. S. Abumansour, and A. Zubiaga, “Automated fact-checking: A survey,” <i>Language and Linguistics Compass</i>, vol. 15, no. 10, p. e12438, 2021, doi: <a href=\"https://doi.org/10.1111/lnc3.12438\">10.1111/lnc3.12438</a>.</div>\n",
    "  </div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/YFIM6W6S\"></i>\n",
    "    <div class=\"csl-left-margin\">[2]</div><div class=\"csl-right-inline\">J. Thorne and A. Vlachos, “Automated Fact Checking: Task Formulations, Methods and Future Directions,” in <i>Proceedings of the 27th International Conference on Computational Linguistics</i>, Santa Fe, New Mexico, USA, Aug. 2018, pp. 3346–3359. Accessed: May 24, 2025. [Online]. Available: <a href=\"https://aclanthology.org/C18-1283/\">https://aclanthology.org/C18-1283/</a></div>\n",
    "  </div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/Z792DB78\"></i>\n",
    "    <div class=\"csl-left-margin\">[3]</div><div class=\"csl-right-inline\">D. Chen, A. Fisch, J. Weston, and A. Bordes, “Reading Wikipedia to Answer Open-Domain Questions,” in <i>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, Vancouver, Canada, Jul. 2017, pp. 1870–1879. doi: <a href=\"https://doi.org/10.18653/v1/P17-1171\">10.18653/v1/P17-1171</a>.</div>\n",
    "  </div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/YS2A7YKH\"></i>\n",
    "    <div class=\"csl-left-margin\">[4]</div><div class=\"csl-right-inline\">K. Spärck Jones, “A Statistical Interpretation of Term Specifity and Its Application in Retrieval,” <i>Journal of Documentation</i>, vol. 28, no. 1, pp. 11–21, Jan. 1972, doi: <a href=\"https://doi.org/10.1108/eb026526\">10.1108/eb026526</a>.</div>\n",
    "  </div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/QPZSI7AS\"></i>\n",
    "    <div class=\"csl-left-margin\">[5]</div><div class=\"csl-right-inline\">S. Robertson, “Understanding inverse document frequency: on theoretical arguments for IDF,” <i>Journal of Documentation</i>, vol. 60, no. 5, pp. 503–520, Jan. 2004, doi: <a href=\"https://doi.org/10.1108/00220410410560582\">10.1108/00220410410560582</a>.</div>\n",
    "  </div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/MDDH62P9\"></i>\n",
    "    <div class=\"csl-left-margin\">[6]</div><div class=\"csl-right-inline\">R. Mihalcea, “Corpus-based and Knowledge-based Measures of Text Semantic Similarity”.</div>\n",
    "  </div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/2P77V7I4\"></i>\n",
    "    <div class=\"csl-left-margin\">[7]</div><div class=\"csl-right-inline\">M. Marcińczuk, M. Gniewkowski, T. Walkowiak, and M. Będkowski, “Text Document Clustering: Wordnet vs. TF-IDF vs. Word Embeddings,” in <i>Proceedings of the 11th Global Wordnet Conference</i>, University of South Africa (UNISA), Jan. 2021, pp. 207–214. Accessed: May 24, 2025. [Online]. Available: <a href=\"https://aclanthology.org/2021.gwc-1.24/\">https://aclanthology.org/2021.gwc-1.24/</a></div>\n",
    "  </div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|79625/MLYXVA9L\"></i>\n",
    "    <div class=\"csl-left-margin\">[8]</div><div class=\"csl-right-inline\">D. Chandrasekaran and V. Mago, “Evolution of Semantic Similarity—A Survey,” <i>ACM Comput. Surv.</i>, vol. 54, no. 2, p. 41:1-41:37, Feb. 2021, doi: <a href=\"https://doi.org/10.1145/3440755\">10.1145/3440755</a>.</div>\n",
    "  </div>\n",
    "</div>\n",
    "<!-- BIBLIOGRAPHY END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3148f98-5bd9-46cf-9a3d-df66a83b4266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "79625/2P77V7I4": {
      "URL": "https://aclanthology.org/2021.gwc-1.24/",
      "abstract": "In the paper, we deal with the problem of unsupervised text document clustering for the Polish language. Our goal is to compare the modern approaches based on language modeling (doc2vec and BERT) with the classical ones, i.e., TF-IDF and wordnet-based. The experiments are conducted on three datasets containing qualification descriptions. The experiments' results showed that wordnet-based similarity measures could compete and even outperform modern embedding-based approaches.",
      "accessed": {
       "date-parts": [
        [
         2025,
         5,
         24
        ]
       ]
      },
      "author": [
       {
        "family": "Marcińczuk",
        "given": "Michał"
       },
       {
        "family": "Gniewkowski",
        "given": "Mateusz"
       },
       {
        "family": "Walkowiak",
        "given": "Tomasz"
       },
       {
        "family": "Będkowski",
        "given": "Marcin"
       }
      ],
      "container-title": "Proceedings of the 11th Global Wordnet Conference",
      "editor": [
       {
        "family": "Vossen",
        "given": "Piek"
       },
       {
        "family": "Fellbaum",
        "given": "Christiane"
       }
      ],
      "event": "GWC 2021",
      "event-place": "University of South Africa (UNISA)",
      "id": "79625/2P77V7I4",
      "issued": {
       "date-parts": [
        [
         2021,
         1
        ]
       ]
      },
      "page": "207–214",
      "publisher": "Global Wordnet Association",
      "publisher-place": "University of South Africa (UNISA)",
      "shortTitle": "Text Document Clustering",
      "system_id": "zotero|79625/2P77V7I4",
      "title": "Text Document Clustering: Wordnet vs. TF-IDF vs. Word Embeddings",
      "type": "paper-conference"
     },
     "79625/MDDH62P9": {
      "abstract": "This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classiﬁcation, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientiﬁc documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method outperforms methods based on simple lexical matching, resulting in up to 13% error rate reduction with respect to the traditional vector-based similarity metric.",
      "author": [
       {
        "family": "Mihalcea",
        "given": "Rada"
       }
      ],
      "id": "79625/MDDH62P9",
      "language": "en",
      "system_id": "zotero|79625/MDDH62P9",
      "title": "Corpus-based and Knowledge-based Measures of Text Semantic Similarity",
      "type": "article-journal"
     },
     "79625/MLYXVA9L": {
      "DOI": "10.1145/3440755",
      "URL": "https://doi.org/10.1145/3440755",
      "abstract": "Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network–based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.",
      "accessed": {
       "date-parts": [
        [
         2025,
         5,
         24
        ]
       ]
      },
      "author": [
       {
        "family": "Chandrasekaran",
        "given": "Dhivya"
       },
       {
        "family": "Mago",
        "given": "Vijay"
       }
      ],
      "container-title": "ACM Comput. Surv.",
      "id": "79625/MLYXVA9L",
      "issue": "2",
      "issued": {
       "date-parts": [
        [
         "2021",
         2,
         18
        ]
       ]
      },
      "page": "41:1–41:37",
      "system_id": "zotero|79625/MLYXVA9L",
      "title": "Evolution of Semantic Similarity—A Survey",
      "type": "article-journal",
      "volume": "54"
     },
     "79625/QPZSI7AS": {
      "DOI": "10.1108/00220410410560582",
      "URL": "https://doi.org/10.1108/00220410410560582",
      "abstract": "The term‐weighting function known as IDF was proposed in 1972, and has since been extremely widely used, usually as part of a TF*IDF function. It is often described as a heuristic, and many papers have been written (some based on Shannon's Information Theory) seeking to establish some theoretical basis for it. Some of these attempts are reviewed, and it is shown that the Information Theory approaches are problematic, but that there are good theoretical justifications of both IDF and TF*IDF in the traditional probabilistic model of information retrieval.",
      "accessed": {
       "date-parts": [
        [
         2025,
         5,
         24
        ]
       ]
      },
      "author": [
       {
        "family": "Robertson",
        "given": "Stephen"
       }
      ],
      "container-title": "Journal of Documentation",
      "id": "79625/QPZSI7AS",
      "issue": "5",
      "issued": {
       "date-parts": [
        [
         2004,
         1,
         1
        ]
       ]
      },
      "note": "Publisher: Emerald Group Publishing Limited",
      "page": "503-520",
      "shortTitle": "Understanding inverse document frequency",
      "system_id": "zotero|79625/QPZSI7AS",
      "title": "Understanding inverse document frequency: on theoretical arguments for IDF",
      "type": "article-journal",
      "volume": "60"
     },
     "79625/YFIM6W6S": {
      "URL": "https://aclanthology.org/C18-1283/",
      "abstract": "The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.",
      "accessed": {
       "date-parts": [
        [
         2025,
         5,
         24
        ]
       ]
      },
      "author": [
       {
        "family": "Thorne",
        "given": "James"
       },
       {
        "family": "Vlachos",
        "given": "Andreas"
       }
      ],
      "container-title": "Proceedings of the 27th International Conference on Computational Linguistics",
      "editor": [
       {
        "family": "Bender",
        "given": "Emily M."
       },
       {
        "family": "Derczynski",
        "given": "Leon"
       },
       {
        "family": "Isabelle",
        "given": "Pierre"
       }
      ],
      "event": "COLING 2018",
      "event-place": "Santa Fe, New Mexico, USA",
      "id": "79625/YFIM6W6S",
      "issued": {
       "date-parts": [
        [
         2018,
         8
        ]
       ]
      },
      "page": "3346–3359",
      "publisher": "Association for Computational Linguistics",
      "publisher-place": "Santa Fe, New Mexico, USA",
      "shortTitle": "Automated Fact Checking",
      "system_id": "zotero|79625/YFIM6W6S",
      "title": "Automated Fact Checking: Task Formulations, Methods and Future Directions",
      "type": "paper-conference"
     },
     "79625/YS2A7YKH": {
      "DOI": "10.1108/eb026526",
      "URL": "https://doi.org/10.1108/eb026526",
      "abstract": "The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently‐occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.",
      "accessed": {
       "date-parts": [
        [
         2025,
         5,
         24
        ]
       ]
      },
      "author": [
       {
        "family": "Spärck Jones",
        "given": "Karen"
       }
      ],
      "container-title": "Journal of Documentation",
      "id": "79625/YS2A7YKH",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         1972,
         1,
         1
        ]
       ]
      },
      "note": "Publisher: MCB UP Ltd",
      "page": "11-21",
      "system_id": "zotero|79625/YS2A7YKH",
      "title": "A Statistical Interpretation of Term Specifity and Its Application in Retrieval",
      "type": "article-journal",
      "volume": "28"
     },
     "79625/Z62QSGRH": {
      "DOI": "10.1111/lnc3.12438",
      "URL": "https://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12438",
      "abstract": "As online false information continues to grow, automated fact-checking has gained an increasing amount of attention in recent years. Researchers in the field of Natural Language Processing (NLP) have contributed to the task by building fact-checking datasets, devising automated fact-checking pipelines and proposing NLP methods to further research in the development of different components. This article reviews relevant research on automated fact-checking covering both the claim detection and claim validation components.",
      "accessed": {
       "date-parts": [
        [
         2025,
         5,
         24
        ]
       ]
      },
      "author": [
       {
        "family": "Zeng",
        "given": "Xia"
       },
       {
        "family": "Abumansour",
        "given": "Amani S."
       },
       {
        "family": "Zubiaga",
        "given": "Arkaitz"
       }
      ],
      "container-title": "Language and Linguistics Compass",
      "id": "79625/Z62QSGRH",
      "issue": "10",
      "issued": {
       "date-parts": [
        [
         2021
        ]
       ]
      },
      "language": "en",
      "note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12438",
      "page": "e12438",
      "shortTitle": "Automated fact-checking",
      "system_id": "zotero|79625/Z62QSGRH",
      "title": "Automated fact-checking: A survey",
      "type": "article-journal",
      "volume": "15"
     },
     "79625/Z792DB78": {
      "DOI": "10.18653/v1/P17-1171",
      "URL": "https://aclanthology.org/P17-1171/",
      "abstract": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
      "accessed": {
       "date-parts": [
        [
         2025,
         5,
         24
        ]
       ]
      },
      "author": [
       {
        "family": "Chen",
        "given": "Danqi"
       },
       {
        "family": "Fisch",
        "given": "Adam"
       },
       {
        "family": "Weston",
        "given": "Jason"
       },
       {
        "family": "Bordes",
        "given": "Antoine"
       }
      ],
      "container-title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "editor": [
       {
        "family": "Barzilay",
        "given": "Regina"
       },
       {
        "family": "Kan",
        "given": "Min-Yen"
       }
      ],
      "event": "ACL 2017",
      "event-place": "Vancouver, Canada",
      "id": "79625/Z792DB78",
      "issued": {
       "date-parts": [
        [
         2017,
         7
        ]
       ]
      },
      "page": "1870–1879",
      "publisher": "Association for Computational Linguistics",
      "publisher-place": "Vancouver, Canada",
      "system_id": "zotero|79625/Z792DB78",
      "title": "Reading Wikipedia to Answer Open-Domain Questions",
      "type": "paper-conference"
     }
    }
   },
   "style": "ieee.csl"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
